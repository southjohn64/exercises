{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl_ex1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcrUb29aMXYEUo4dtlzFCl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/southjohn64/exercises/blob/main/dl_ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBMXlYxKuMi3"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "def initialize_parameters(layer_dims):\r\n",
        "    '''\r\n",
        "    :param layer_dims:  an array of the dimensions of each layer in the network\r\n",
        "    (layer 0 is the size of the flattened input, layer L is the output softmax)\r\n",
        "    :return: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL)\r\n",
        "    '''\r\n",
        "    w_b_parmas_dic = {}\r\n",
        "    for i in range(1, len(layer_dims)):\r\n",
        "        layer_dim_prev = layer_dims[i - 1]\r\n",
        "        layer_dim = layer_dims[i]\r\n",
        "        w_b_parmas_dic['W' + str(i)] = np.random.randn(layer_dim_prev, layer_dim)\r\n",
        "        w_b_parmas_dic['b' + str(i)] = np.zeros((layer_dim, 1))\r\n",
        "\r\n",
        "    return w_b_parmas_dic\r\n",
        "\r\n",
        "\r\n",
        "def linear_forward(A, W, b):\r\n",
        "    '''\r\n",
        "        Implement the linear part of a layer's forward propagation\r\n",
        "        :param A: the activations of the previous layer\r\n",
        "        :param W: the weight matrix of the current layer (of shape [size of current layer, size of previous layer])\r\n",
        "        :param b: the bias vector of the current layer (of shape [size of current layer, 1])\r\n",
        "        :return: Z – the linear component of the activation function (i.e., the value before applying the non-linear function)\r\n",
        "        linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)\r\n",
        "    '''\r\n",
        "\r\n",
        "    # A is in fact the original X features after activations, so layer 1 get the original X vector (as there is no\r\n",
        "    # activation yet)\r\n",
        "    Z = np.dot(W.T, A) + b\r\n",
        "    linear_cache = {'A': A, 'W': W, 'b': b}\r\n",
        "    return Z, linear_cache\r\n",
        "\r\n",
        "\r\n",
        "def softmax(Z):\r\n",
        "    '''\r\n",
        "\r\n",
        "    :param Z:the linear component of the activation function\r\n",
        "    :return:\r\n",
        "    '''\r\n",
        "    nominator = np.exp(Z)\r\n",
        "    denominator = (np.sum(np.exp(Z)))\r\n",
        "\r\n",
        "    A = nominator/ denominator\r\n",
        "    activation_cache = Z\r\n",
        "    return A, activation_cache\r\n",
        "\r\n",
        "\r\n",
        "def relu(Z):\r\n",
        "    '''\r\n",
        "\r\n",
        "    :param Z:the linear component of the activation function\r\n",
        "    :return: A – the activations of the layer\r\n",
        "    activation_cache – returns Z, which will be useful for the backpropagation\r\n",
        "\r\n",
        "    '''\r\n",
        "\r\n",
        "    A = np.maximum(Z, 0)\r\n",
        "    activation_cache = Z\r\n",
        "\r\n",
        "    return A, activation_cache\r\n",
        "\r\n",
        "def\tlinear_activation_forward(A_prev, W, B, activation):\r\n",
        "  Z_current, linear_cache = linear_forward(A_prev, W, B)\r\n",
        "\r\n",
        "  if activation == 'softmax':\r\n",
        "    A_current, activation_cache = softmax(Z_current)\r\n",
        "  elif activation == 'relu':\r\n",
        "    A_current, activation_cache = relu(Z_current)\r\n",
        "  cache =  linear_cache.update(activation_cache)\r\n",
        "  return A_current, cache\r\n",
        "\r\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2fd-yJzueI6"
      },
      "source": [
        " nets = initialize_parameters([4, 6, 6, 10])\r\n",
        "a = np.random.uniform(0, 255, (4,1))  # mnist image\r\n",
        "print('a ',a)\r\n",
        "\r\n",
        "z, linear_cache = linear_forward(a, nets.get('W1'), nets.get('b1'))\r\n",
        "relu(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WnfkuC56H3t",
        "outputId": "f5ced494-4246-4c18-c0f0-c5c964d7b572"
      },
      "source": [
        "nets = initialize_parameters([2, 4, 10])\r\n",
        "#a = np.random.uniform(0, 255, 4).reshape(-1, 1)  # mnist image\r\n",
        "#a = np.random.uniform(0, 255, (4,1))  # mnist image\r\n",
        "a = np.array([[1],[2]])\r\n",
        "print(a.shape)\r\n",
        "\r\n",
        "z, linear_cache = linear_forward(a, nets.get('W1'), nets.get('b1'))\r\n",
        "print('z: ', z)\r\n",
        "print('linear_cache: ', linear_cache)\r\n",
        "#A_current, activation_cache = relu(Z_current)\r\n",
        "\r\n",
        "#linear_activation_forward(a, nets.get('W1'), nets.get('b1'))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 1)\n",
            "z:  [[-2.86420238]\n",
            " [-4.82335325]\n",
            " [-4.45644025]\n",
            " [ 0.63992757]]\n",
            "linear_cache:  {'A': array([[1],\n",
            "       [2]]), 'W': array([[-1.18030014, -1.98778723, -2.61150807, -1.20035104],\n",
            "       [-0.84195112, -1.41778301, -0.92246609,  0.92013931]]), 'b': array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]])}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}